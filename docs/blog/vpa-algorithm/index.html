<!doctype html><html lang=en-us><head><meta http-equiv=X-Clacks-Overhead content="GNU Terry Pratchett"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>K8s Vertical Pod Autoscaler's Algorithm | Tanay Tummalapalli</title>
<meta name=title content="K8s Vertical Pod Autoscaler's Algorithm"><meta name=description content="A look at the Vertical Pod Autoscaler's Algorithm. Documentation of the VPA algorithm"><meta name=keywords content="algorithms,kubernetes,vertical-pod-autoscaler,"><meta property="og:url" content="https://ttanay.github.io/blog/vpa-algorithm/"><meta property="og:site_name" content="Tanay Tummalapalli"><meta property="og:title" content="K8s Vertical Pod Autoscaler's Algorithm"><meta property="og:description" content="A look at the Vertical Pod Autoscaler's Algorithm. Documentation of the VPA algorithm"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2024-08-06T00:00:00+00:00"><meta property="article:modified_time" content="2024-08-06T00:00:00+00:00"><meta property="article:tag" content="Algorithms"><meta property="article:tag" content="Kubernetes"><meta property="article:tag" content="Vertical-Pod-Autoscaler"><meta name=twitter:card content="summary"><meta name=twitter:title content="K8s Vertical Pod Autoscaler's Algorithm"><meta name=twitter:description content="A look at the Vertical Pod Autoscaler's Algorithm. Documentation of the VPA algorithm"><meta itemprop=name content="K8s Vertical Pod Autoscaler's Algorithm"><meta itemprop=description content="A look at the Vertical Pod Autoscaler's Algorithm. Documentation of the VPA algorithm"><meta itemprop=datePublished content="2024-08-06T00:00:00+00:00"><meta itemprop=dateModified content="2024-08-06T00:00:00+00:00"><meta itemprop=wordCount content="1419"><meta itemprop=keywords content="Algorithms,Kubernetes,Vertical-Pod-Autoscaler"><meta name=referrer content="no-referrer-when-downgrade"><style>body{font-family:Verdana,sans-serif;margin:auto;padding:20px;max-width:720px;text-align:left;background-color:#fff;word-wrap:break-word;overflow-wrap:break-word;line-height:1.5;color:#444}h1,h2,h3,h4,h5,h6,strong,b{color:#222}a{color:#3273dc}.title{text-decoration:none;border:0}.title span{font-weight:400}nav a{margin-right:10px}textarea{width:100%;font-size:16px}input{font-size:16px}content{line-height:1.6}table{width:100%}img{max-width:100%}code{padding:2px 5px;background-color:#f2f2f2}pre code{color:#222;display:block;padding:20px;white-space:pre-wrap;font-size:14px;overflow-x:auto}div.highlight pre{background-color:initial;color:initial}div.highlight code{background-color:unset;color:unset}blockquote{border-left:1px solid #999;color:#222;padding-left:20px;font-style:italic}footer{padding:25px;text-align:center}.helptext{color:#777;font-size:small}.errorlist{color:#eba613;font-size:small}ul.blog-posts{list-style-type:none;padding:unset}ul.blog-posts li{display:flex}ul.blog-posts li span{flex:0 0 130px}ul.blog-posts li a:visited{color:#8b6fcb}@media(prefers-color-scheme:dark){body{background-color:#333;color:#ddd}h1,h2,h3,h4,h5,h6,strong,b{color:#eee}a{color:#8cc2dd}code{background-color:#777}pre code{color:#ddd}blockquote{color:#ccc}textarea,input{background-color:#252525;color:#ddd}.helptext{color:#aaa}}</style></head><body><header><a href=/ class=title><h2>Tanay Tummalapalli</h2></a><nav><a href=/>Home</a>
<a href=/blog>Blog</a></nav></header><main><h1>K8s Vertical Pod Autoscaler's Algorithm</h1><p><i><time datetime=2024-08-06 pubdate>06 Aug, 2024</time></i></p><content><p>I tried to understand how the Vertical Pod Autoscaler(VPA) works and found a comment<a href=https://github.com/kubernetes/autoscaler/issues/2747#issuecomment-616037197>[1]</a> by this gentleman(<a href=%5Bhttps://github.com/yashbhutwala%5D>@yashbutwala</a>) explaining in brief how it works:</p><blockquote><p>recommendations are calculated using decaying histogram of weighted samples from the metrics server, where the newer samples are assigned higher weights; older samples are decaying and hence affect less and less w.r.t. to the recommendations. CPU is calculated using the 90th percentile of all cpu samples, and memory is calculated using the 90th percentile peak over the 8 day window</p></blockquote><p>While digging through the VPA codebase, I tried to get at the essence of the recommender&rsquo;s algorithm.
This is my attempt to document my digging.</p><p><strong>Note:</strong> This post is latest as on <a href=https://github.com/kubernetes/autoscaler/tree/402ea4176fea622ebb2279ada1f94232705de400/vertical-pod-autoscaler><code>402ea4176</code></a>.</p><ul><li><a href=#background>Background</a></li><li><a href=#the-algorithm>The algorithm</a><ul><li><a href=#raw-signal>Raw Signal</a></li><li><a href=#the-histogram>The Histogram</a><ul><li><a href=#exponential-bucketing-scheme>Exponential bucketing scheme</a></li><li><a href=#percentiles>Percentiles</a></li></ul></li><li><a href=#the-weight-of-a-sample>The weight of a sample</a><ul><li><a href=#exponential-decay-multiplier>Exponential Decay Multiplier</a></li><li><a href=#load-adjusted-cpu-usage-multiplier>Load-adjusted CPU usage multiplier</a></li></ul></li><li><a href=#safety-margin>Safety margin</a></li><li><a href=#confidence-multiplier>Confidence Multiplier</a></li><li><a href=#upper-bound-and-lower-bound>Upper bound and Lower bound</a></li><li><a href=#minimum-resources>Minimum Resources</a></li></ul></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#references>References</a></li></ul><h2 id=background>Background</h2><p>The VPA solves the problem of optimal resource allocation for CPU and Memory.
The solution space for this problem takes a few different approaches<a href=https://pdfs.semanticscholar.org/74d8/8b2c2bbba11c42439ca9184c8f90f53c43fe.pdf>[2]</a>:</p><ol><li>Threshold-based rules</li><li>Re-inforcement Learning</li><li>Queueing Theory</li><li>Time-series analysis</li><li>Control Theory</li></ol><p>The VPA takes the Time-series analysis approach to solving the problem.</p><p>I don&rsquo;t go into more high-level detail about the VPA in this post.<br><strong>TL;DR</strong>: It reads metrics from Kubernetes&rsquo; metrics server and analyzes samples from time-series for recommending resources.</p><p>I highly recommend reading Google&rsquo;s paper on autoscalers for their clusters<a href=https://research.google/pubs/autopilot-workload-autoscaling-at-google-scale/>[3]</a> as it seems to form the basis for the VPA&rsquo;s design.
I refer to it as the &ldquo;reference paper&rdquo; in this post.</p><h2 id=the-algorithm>The algorithm</h2><p>In essence, the vertical pod autoscaler&rsquo;s algorithm aggregates the sample data collected to compute a recommendation.
This section is structured to present the individual abstractions that the algorithm can be thought of as composed of in their order of application.</p><h3 id=raw-signal>Raw Signal</h3><p>The raw signal is composed of samples of the resource usage captured at given intervals.<br>For CPU, simply a CPU usage sample is considered. It corresponds to the metric <code>container_cpu_usage_seconds_total</code>.
For Memory, the maximum sample value is considered for a given aggregation interval. It corresponds to the metric <code>container_memory_working_set_bytes</code>.</p><p>See <a href=https://github.com/kubernetes/autoscaler/blob/402ea4176fea622ebb2279ada1f94232705de400/vertical-pod-autoscaler/pkg/recommender/input/history/history_provider.go#L286-L322>reference location</a>.</p><h3 id=the-histogram>The Histogram</h3><p>The raw resource usage signal is then aggregated into a histogram to save space.
The Histogram partitions the input metric(resource usage) into buckets.
The bucket&rsquo;s value is the associated weight of the sample. (We get into this in detail in the next section).
To keep things simple, we consider the weight of the sample to be the number of ocurrences of a sample for now.</p><p>See <a href=https://github.com/kubernetes/autoscaler/blob/402ea4176fea622ebb2279ada1f94232705de400/vertical-pod-autoscaler/pkg/recommender/util/histogram.go#L85-L104>reference location</a></p><h4 id=exponential-bucketing-scheme>Exponential bucketing scheme</h4><p>Generally, the buckets of a histogram are partitioned equally.
Eg: For a range \([1, 10]\) with \(5\) buckets, each bucket would contain \(2\) values.
However, for a large range of values, a large number of buckets may be sparsely populated.
To avoid that, the VPA uses a histogram with an exponential bucketing scheme where the bucket sizes grow exponentially.
This allows for having a lower granularity for extremely large values while having a high granularity for smaller and more likely values.</p><p><a href=https://github.com/kubernetes/autoscaler/blob/402ea4176fea622ebb2279ada1f94232705de400/vertical-pod-autoscaler/pkg/recommender/util/histogram_options.go#L53-L62>This comment</a> captures it well:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#75715e>// NewExponentialHistogramOptions returns HistogramOptions describing a
</span></span></span><span style=display:flex><span><span style=color:#75715e>// histogram with exponentially growing bucket boundaries. The first bucket
</span></span></span><span style=display:flex><span><span style=color:#75715e>// covers the range [0..firstBucketSize). Bucket with index n has size equal to firstBucketSize * ratio^n.
</span></span></span><span style=display:flex><span><span style=color:#75715e>// It follows that the bucket with index n &gt;= 1 starts at:
</span></span></span><span style=display:flex><span><span style=color:#75715e>//
</span></span></span><span style=display:flex><span><span style=color:#75715e>//	firstBucketSize * (1 + ratio + ratio^2 + ... + ratio^(n-1)) =
</span></span></span><span style=display:flex><span><span style=color:#75715e>//	firstBucketSize * (ratio^n - 1) / (ratio - 1).
</span></span></span><span style=display:flex><span><span style=color:#75715e>//
</span></span></span><span style=display:flex><span><span style=color:#75715e>// The last bucket start is larger or equal to maxValue.
</span></span></span></code></pre></div><p>It also uses a config option named <code>espilon</code> but we don&rsquo;t need to know about it for the purpose of this post.</p><h4 id=percentiles>Percentiles</h4><p>The percentile of a histogram is defined as the weight of the bucket that contains the \(p\)-th percentile value.
This gives us a way to approximately compute a percentile.</p><p>It is <a href=https://github.com/kubernetes/autoscaler/blob/402ea4176fea622ebb2279ada1f94232705de400/vertical-pod-autoscaler/pkg/recommender/util/histogram.go#L159-L179>computed</a> as:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#66d9ef>func</span> (<span style=color:#a6e22e>h</span> <span style=color:#f92672>*</span><span style=color:#a6e22e>histogram</span>) <span style=color:#a6e22e>Percentile</span>(<span style=color:#a6e22e>percentile</span> <span style=color:#66d9ef>float64</span>) <span style=color:#66d9ef>float64</span> {
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>if</span> <span style=color:#a6e22e>h</span>.<span style=color:#a6e22e>IsEmpty</span>() {
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>return</span> <span style=color:#ae81ff>0.0</span>
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>partialSum</span> <span style=color:#f92672>:=</span> <span style=color:#ae81ff>0.0</span>
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>threshold</span> <span style=color:#f92672>:=</span> <span style=color:#a6e22e>percentile</span> <span style=color:#f92672>*</span> <span style=color:#a6e22e>h</span>.<span style=color:#a6e22e>totalWeight</span>
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>bucket</span> <span style=color:#f92672>:=</span> <span style=color:#a6e22e>h</span>.<span style=color:#a6e22e>minBucket</span>
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>for</span> ; <span style=color:#a6e22e>bucket</span> &lt; <span style=color:#a6e22e>h</span>.<span style=color:#a6e22e>maxBucket</span>; <span style=color:#a6e22e>bucket</span><span style=color:#f92672>++</span> {
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>partialSum</span> <span style=color:#f92672>+=</span> <span style=color:#a6e22e>h</span>.<span style=color:#a6e22e>bucketWeight</span>[<span style=color:#a6e22e>bucket</span>]
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>if</span> <span style=color:#a6e22e>partialSum</span> <span style=color:#f92672>&gt;=</span> <span style=color:#a6e22e>threshold</span> {
</span></span><span style=display:flex><span>			<span style=color:#66d9ef>break</span>
</span></span><span style=display:flex><span>		}
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>if</span> <span style=color:#a6e22e>bucket</span> &lt; <span style=color:#a6e22e>h</span>.<span style=color:#a6e22e>options</span>.<span style=color:#a6e22e>NumBuckets</span>()<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span> {
</span></span><span style=display:flex><span>		<span style=color:#75715e>// Return the end of the bucket.
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>		<span style=color:#66d9ef>return</span> <span style=color:#a6e22e>h</span>.<span style=color:#a6e22e>options</span>.<span style=color:#a6e22e>GetBucketStart</span>(<span style=color:#a6e22e>bucket</span> <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	<span style=color:#75715e>// Return the start of the last bucket (note that the last bucket
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	<span style=color:#75715e>// doesn&#39;t have an upper bound).
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	<span style=color:#66d9ef>return</span> <span style=color:#a6e22e>h</span>.<span style=color:#a6e22e>options</span>.<span style=color:#a6e22e>GetBucketStart</span>(<span style=color:#a6e22e>bucket</span>)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h3 id=the-weight-of-a-sample>The weight of a sample</h3><p>In a bare histogram, the weight of a bucket is it&rsquo;s ocurrence.
For more sophisticated use-cases, the weight assigned can be tweaked to introduce a &ldquo;bias&rdquo;.
The recommender algorithm introduces the following biases:</p><ol><li>Exponential Decay multiplier</li><li>Load-Adjusted CPU usage multiplier</li></ol><h4 id=exponential-decay-multiplier>Exponential Decay Multiplier</h4><p>The Google Autopilot paper mentions the following rationale behind introducing an exponential decay multiplier:</p><blockquote><p>We want the limits to increase swiftly in response to rising
usage, but reduce slowly after the load decreases to avoid a
too-rapid response to temporary downward workload fluc-
tuations.</p></blockquote><p>To &ldquo;bias&rdquo; the weight of the histogram&rsquo;s samples towards more recent samples, the weight of a sample is multiplied by an exponentially decaying multiplier:</p>$$2^{\Large {\frac{t - t_0}{\lambda}}}$$<p>where \(t - t_0\) is the relative age of given sample with respect to a reference timestamp,
and \(\lambda\) is the half-life(24 hours by default).</p><p>See <a href=https://github.com/kubernetes/autoscaler/blob/402ea4176fea622ebb2279ada1f94232705de400/vertical-pod-autoscaler/pkg/recommender/util/decaying_histogram.go#L108-L118>reference location</a></p><h4 id=load-adjusted-cpu-usage-multiplier>Load-adjusted CPU usage multiplier</h4><p>The reference paper(define properly) mentions the following rationale for using load-adjusted weights for CPU:</p><blockquote><p>In many cases, we want to ensure that a given
percentile of the offered load can be served when the limit
is set to accommodate the offered load, rather than simply
a count of times that instantaneous observed load can be
handled â€“ i.e, we want to weight the calculation by the load,
not the sample count.</p></blockquote><p>A <a href=https://github.com/kubernetes/autoscaler/blob/402ea4176fea622ebb2279ada1f94232705de400/vertical-pod-autoscaler/pkg/recommender/model/aggregate_container_state.go#L207-L222>comment in code</a> says the following:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#75715e>// Samples are added with the weight equal to the current request. This means that
</span></span></span><span style=display:flex><span><span style=color:#75715e>// whenever the request is increased, the history accumulated so far effectively decays,
</span></span></span><span style=display:flex><span><span style=color:#75715e>// which helps react quickly to CPU starvation.
</span></span></span></code></pre></div><p>The problem is that the usage needs to be considered in the context of the resources available.
If a process was consuming 2 vCPUs when 2 vCPUs were requested, but, later consumes 2 vCPUs with 8 vCPUs requested,
a simple percentile won&rsquo;t capture this.
To get closer to a &ldquo;utilization&rdquo;-like metric, the CPU requested is considered to be the weight of a sample.</p><p>See <a href=https://github.com/kubernetes/autoscaler/blob/402ea4176fea622ebb2279ada1f94232705de400/vertical-pod-autoscaler/pkg/recommender/model/aggregate_container_state.go#L207-L222>reference location</a></p><p>This figure in the reference paper highlights this point visually:
<img src=./vpa_load_adjusted_cpu_figure.png alt="Load-adjusted CPU usage figure"></p><h3 id=safety-margin>Safety margin</h3><p>This is simply a %age margin that the recommended request is scaled by for safety.</p>$$recommendation = (1 + safetyMargin) \times recommendation$$<p>See <a href=https://github.com/kubernetes/autoscaler/blob/402ea4176fea622ebb2279ada1f94232705de400/vertical-pod-autoscaler/pkg/recommender/logic/estimator.go#L140-L148>reference location</a></p><h3 id=confidence-multiplier>Confidence Multiplier</h3><p>As the VPA recommends resources based on historical resource usage,
if the available data is negligible, the recommendation isn&rsquo;t likely to be correct.
Therefore, the confidence multiplier was introduced.
From a <a href=https://github.com/kubernetes/autoscaler/blob/402ea4176fea622ebb2279ada1f94232705de400/vertical-pod-autoscaler/pkg/recommender/logic/recommender.go#L118-L128>comment in code</a>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#75715e>// ...  This means
</span></span></span><span style=display:flex><span><span style=color:#75715e>// that the updater will be less eager to evict pods with short history
</span></span></span><span style=display:flex><span><span style=color:#75715e>// in order to reclaim unused resources.
</span></span></span></code></pre></div><p>The confidence multiplier is a heuristic defined as:</p>$$\Biggl[{1 + {\frac{multiplier}{confidence}}} \Biggr] ^ {exponent}$$<p>\(multiplier\) and \(exponent\) are heuristic values specified statically in code.</p><p>\(confidence\) is a measure of the available historical data. It is defined as:</p>$$confidence = min(lifespan, samplesAmount)$$<p>where \(lifespan = t_n - t_0\) measured in days,
and \(samplesAmount = {\large \frac{numSamples}{60 \times 24} }\) such that it represents the number of days for which samples are available assuming a rate of 1 sample/minute.</p><p>See <a href=https://github.com/kubernetes/autoscaler/blob/402ea4176fea622ebb2279ada1f94232705de400/vertical-pod-autoscaler/pkg/recommender/logic/estimator.go#L106-L119>reference location</a></p><h3 id=upper-bound-and-lower-bound>Upper bound and Lower bound</h3><p>The VPA recommender computes three recommendations with different settings - Lower bound, Target, and Upper bound.
The updater component of the VPA uses this update a pod if the resource request is outside the range \((lowerBound, upperBound)\).</p><p>See <a href=https://github.com/kubernetes/autoscaler/blob/402ea4176fea622ebb2279ada1f94232705de400/vertical-pod-autoscaler/pkg/updater/priority/update_priority_calculator.go#L121-L124>reference location</a></p><h3 id=minimum-resources>Minimum Resources</h3><p>The recommender imposes a minimum value for both resources - CPU and Memory.</p><p>See <a href=https://github.com/kubernetes/autoscaler/blob/402ea4176fea622ebb2279ada1f94232705de400/vertical-pod-autoscaler/pkg/recommender/logic/recommender.go#L76-L80>reference location</a></p><h2 id=conclusion>Conclusion</h2><p>To understand the algorithm, I think the final piece of the puzzle is the considerations for choosing a suitable algorithm that the reference paper mentions.
For memory, the risk of using an incorrect recommendation can lead to OOM errors. For critical workloads, the maximum peak memory usage is considered to avoid any disruption due to an OOM. For less critical ones, percentiles from \(P_{98}\) to even lower percentiles can be used.
For CPU, the risk is much less grave as an incorrect recommendation can result in CPU throttling in the worst case. Thus, we can use a percentile(eg: \(P_{95}\)) or even average if the workload is not CPU-bound.</p><h2 id=references>References</h2><ol><li><a href=https://github.com/kubernetes/autoscaler/issues/2747#issuecomment-616037197>https://github.com/kubernetes/autoscaler/issues/2747#issuecomment-616037197</a></li><li><a href=https://pdfs.semanticscholar.org/74d8/8b2c2bbba11c42439ca9184c8f90f53c43fe.pdf>https://pdfs.semanticscholar.org/74d8/8b2c2bbba11c42439ca9184c8f90f53c43fe.pdf</a></li><li><a href=https://research.google/pubs/autopilot-workload-autoscaling-at-google-scale/>https://research.google/pubs/autopilot-workload-autoscaling-at-google-scale/</a></li></ol></content><p><a href=https://ttanay.github.io/tags/algorithms/>#Algorithms</a>
<a href=https://ttanay.github.io/tags/kubernetes/>#Kubernetes</a>
<a href=https://ttanay.github.io/tags/vertical-pod-autoscaler/>#Vertical-Pod-Autoscaler</a></p></main><footer></footer><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script>MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["\\(","\\)"]]}}</script></body></html>