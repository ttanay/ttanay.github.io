<!doctype html><html lang=en-us><head><meta http-equiv=X-Clacks-Overhead content="GNU Terry Pratchett"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>K8s Vertical Pod Autoscaler's Algorithm | Tanay Tummalapalli</title>
<meta name=title content="K8s Vertical Pod Autoscaler's Algorithm"><meta name=description content="A look at the Vertical Pod Autoscaler's Algorithm. Documentation of the VPA algorithm"><meta name=keywords content="algorithms,kubernetes,vertical-pod-autoscaler,"><meta property="og:url" content="https://ttanay.github.io/blog/vpa-algorithm/"><meta property="og:site_name" content="Tanay Tummalapalli"><meta property="og:title" content="K8s Vertical Pod Autoscaler's Algorithm"><meta property="og:description" content="A look at the Vertical Pod Autoscaler's Algorithm. Documentation of the VPA algorithm"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2024-08-06T00:00:00+00:00"><meta property="article:modified_time" content="2024-08-06T00:00:00+00:00"><meta property="article:tag" content="Algorithms"><meta property="article:tag" content="Kubernetes"><meta property="article:tag" content="Vertical-Pod-Autoscaler"><meta name=twitter:card content="summary"><meta name=twitter:title content="K8s Vertical Pod Autoscaler's Algorithm"><meta name=twitter:description content="A look at the Vertical Pod Autoscaler's Algorithm. Documentation of the VPA algorithm"><meta itemprop=name content="K8s Vertical Pod Autoscaler's Algorithm"><meta itemprop=description content="A look at the Vertical Pod Autoscaler's Algorithm. Documentation of the VPA algorithm"><meta itemprop=datePublished content="2024-08-06T00:00:00+00:00"><meta itemprop=dateModified content="2024-08-06T00:00:00+00:00"><meta itemprop=wordCount content="1198"><meta itemprop=keywords content="Algorithms,Kubernetes,Vertical-Pod-Autoscaler"><meta name=referrer content="no-referrer-when-downgrade"><style>body{font-family:Verdana,sans-serif;margin:auto;padding:20px;max-width:720px;text-align:left;background-color:#fff;word-wrap:break-word;overflow-wrap:break-word;line-height:1.5;color:#444}h1,h2,h3,h4,h5,h6,strong,b{color:#222}a{color:#3273dc}.title{text-decoration:none;border:0}.title span{font-weight:400}nav a{margin-right:10px}textarea{width:100%;font-size:16px}input{font-size:16px}content{line-height:1.6}table{width:100%}img{max-width:100%}code{padding:2px 5px;background-color:#f2f2f2}pre code{color:#222;display:block;padding:20px;white-space:pre-wrap;font-size:14px;overflow-x:auto}div.highlight pre{background-color:initial;color:initial}div.highlight code{background-color:unset;color:unset}blockquote{border-left:1px solid #999;color:#222;padding-left:20px;font-style:italic}footer{padding:25px;text-align:center}.helptext{color:#777;font-size:small}.errorlist{color:#eba613;font-size:small}ul.blog-posts{list-style-type:none;padding:unset}ul.blog-posts li{display:flex}ul.blog-posts li span{flex:0 0 130px}ul.blog-posts li a:visited{color:#8b6fcb}@media(prefers-color-scheme:dark){body{background-color:#333;color:#ddd}h1,h2,h3,h4,h5,h6,strong,b{color:#eee}a{color:#8cc2dd}code{background-color:#777}pre code{color:#ddd}blockquote{color:#ccc}textarea,input{background-color:#252525;color:#ddd}.helptext{color:#aaa}}</style></head><body><header><a href=/ class=title><h2>Tanay Tummalapalli</h2></a><nav><a href=/>Home</a>
<a href=/blog>Blog</a></nav></header><main><h1>K8s Vertical Pod Autoscaler's Algorithm</h1><p><i><time datetime=2024-08-06 pubdate>06 Aug, 2024</time></i></p><content><p>I tried to understand how the Vertical Pod Autoscaler(VPA) works and found a comment<a href=https://github.com/kubernetes/autoscaler/issues/2747#issuecomment-616037197>[1]</a> by this gentleman(<a href=https://github.com/yashbhutwala>@yashbutwala</a>) explaining in brief how it works:</p><blockquote><p>recommendations are calculated using decaying histogram of weighted samples from the metrics server, where the newer samples are assigned higher weights; older samples are decaying and hence affect less and less w.r.t. to the recommendations. CPU is calculated using the 90th percentile of all cpu samples, and memory is calculated using the 90th percentile peak over the 8 day window</p></blockquote><p>I wanted to dig deeper to get at the essence of the recommender&rsquo;s algorithm.
This is my attempt to document my digging.</p><p><strong>Note:</strong> This post is latest as on <a href=https://github.com/kubernetes/autoscaler/tree/402ea4176fea622ebb2279ada1f94232705de400/vertical-pod-autoscaler><code>402ea4176</code></a>.</p><ul><li><a href=#background>Background</a></li><li><a href=#the-algorithm>The algorithm</a><ul><li><a href=#raw-signal>Raw Signal</a></li><li><a href=#the-histogram>The Histogram</a><ul><li><a href=#exponential-bucketing-scheme>Exponential bucketing scheme</a></li><li><a href=#percentiles>Percentiles</a></li></ul></li><li><a href=#the-weight-of-a-sample>The weight of a sample</a><ul><li><a href=#exponential-decay-multiplier>Exponential Decay Multiplier</a></li><li><a href=#load-adjusted-cpu-usage-multiplier>Load-adjusted CPU usage multiplier</a></li></ul></li><li><a href=#safety-margin>Safety margin</a></li><li><a href=#confidence-multiplier>Confidence Multiplier</a></li><li><a href=#upper-bound-and-lower-bound>Upper bound and Lower bound</a></li><li><a href=#minimum-resources>Minimum Resources</a></li></ul></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#references>References</a></li></ul><h2 id=background>Background</h2><p>The VPA solves the problem of optimal resource allocation for CPU and Memory.
The solution space for this problem takes a few different approaches<a href=https://pdfs.semanticscholar.org/74d8/8b2c2bbba11c42439ca9184c8f90f53c43fe.pdf>[2]</a>:</p><ol><li>Threshold-based rules</li><li>Re-inforcement Learning</li><li>Queueing Theory</li><li>Time-series analysis</li><li>Control Theory</li></ol><p>The VPA takes the Time-series analysis approach to solving the problem.</p><p>I don&rsquo;t go into more high-level detail about the VPA in this post.<br><strong>TL;DR</strong>: It reads metrics from Kubernetes&rsquo; metrics server and analyzes samples from time-series for recommending resources.<br>This talk is a really good introduction to the VPA:<br><iframe width=560 height=315 src="https://www.youtube.com/embed/Y4vnYaqhS74?si=CYzCB_99weeHtJia" title="YouTube video player" frameborder=0 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy=strict-origin-when-cross-origin allowfullscreen></iframe></p><p>I highly recommend reading Google&rsquo;s paper on autoscalers for their clusters<a href=https://research.google/pubs/autopilot-workload-autoscaling-at-google-scale/>[3]</a> as it seems to form the basis for the VPA&rsquo;s design.
I refer to it as the &ldquo;reference paper&rdquo; in this post.</p><h2 id=the-algorithm>The algorithm</h2><p>In essence, the vertical pod autoscaler&rsquo;s algorithm aggregates the sample data collected to compute a recommendation.
This section is structured to present the individual abstractions that the algorithm can be thought of as composed of in order of application.</p><h3 id=raw-signal>Raw Signal</h3><p>The raw signal is composed of samples of the resource usage captured at given intervals.<br>For CPU, simply a CPU usage sample is considered. It corresponds to the metric <code>container_cpu_usage_seconds_total</code>.
For Memory, the maximum sample value is considered for a given aggregation interval(default=24h). It corresponds to the metric <code>container_memory_working_set_bytes</code>.</p><iframe frameborder=0 scrolling=no style=width:100%;height:352px allow=clipboard-write src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fkubernetes%2Fautoscaler%2Fblob%2F402ea4176fea622ebb2279ada1f94232705de400%2Fvertical-pod-autoscaler%2Fpkg%2Frecommender%2Finput%2Fhistory%2Fhistory_provider.go%23L298-L310&style=atom-one-dark-reasonable&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe><h3 id=the-histogram>The Histogram</h3><p>The raw resource usage signal is then aggregated into a histogram to save space.
The Histogram partitions the input metric(resource usage) into buckets.
The bucket&rsquo;s value is the associated weight of the sample. (We get into this in detail in the next section).
To keep things simple, we consider the weight of the sample to be the number of ocurrences of a sample for now.</p><iframe frameborder=0 scrolling=no style=width:100%;height:499px allow=clipboard-write src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fkubernetes%2Fautoscaler%2Fblob%2F402ea4176fea622ebb2279ada1f94232705de400%2Fvertical-pod-autoscaler%2Fpkg%2Frecommender%2Futil%2Fhistogram.go%23L85-L104&style=atom-one-dark-reasonable&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe><h4 id=exponential-bucketing-scheme>Exponential bucketing scheme</h4><p>Generally, the buckets of a histogram are partitioned equally.
Eg: For a range \([1, 10]\) with \(5\) buckets, each bucket would contain \(2\) values.
However, for a large range of values, a large number of buckets may be sparsely populated.
To avoid that, the VPA uses a histogram with an exponential bucketing scheme where the bucket sizes grow exponentially.
This allows for having a lower granularity for extremely large values while having a high granularity for smaller and more likely values.</p><p><a href=https://github.com/kubernetes/autoscaler/blob/402ea4176fea622ebb2279ada1f94232705de400/vertical-pod-autoscaler/pkg/recommender/util/histogram_options.go#L53-L62>This comment</a> captures it well:
<iframe frameborder=0 scrolling=no style=width:100%;height:289px allow=clipboard-write src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fkubernetes%2Fautoscaler%2Fblob%2F402ea4176fea622ebb2279ada1f94232705de400%2Fvertical-pod-autoscaler%2Fpkg%2Frecommender%2Futil%2Fhistogram_options.go%23L53-L62&style=atom-one-dark-reasonable&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe></p><p>It also uses a config option named <code>espilon</code> but we don&rsquo;t need to know about it for the purpose of this post.</p><p>Default ratio: <a href=https://github.com/kubernetes/autoscaler/blob/402ea4176fea622ebb2279ada1f94232705de400/vertical-pod-autoscaler/pkg/recommender/model/aggregations_config.go#L71-L72>1.05 i.e. 5% larger</a></p><h4 id=percentiles>Percentiles</h4><p>The percentile of a histogram is defined as the weight of the bucket that contains the \(p\)-th percentile value.
This gives us a way to approximately compute a percentile.</p><p>It is <a href=https://github.com/kubernetes/autoscaler/blob/402ea4176fea622ebb2279ada1f94232705de400/vertical-pod-autoscaler/pkg/recommender/util/histogram.go#L159-L179>computed</a> as:</p><iframe frameborder=0 scrolling=no style=width:100%;height:520px allow=clipboard-write src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fkubernetes%2Fautoscaler%2Fblob%2F402ea4176fea622ebb2279ada1f94232705de400%2Fvertical-pod-autoscaler%2Fpkg%2Frecommender%2Futil%2Fhistogram.go%23L159-L179&style=atom-one-dark-reasonable&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe><p>Default percentile: <a href=https://github.com/kubernetes/autoscaler/blob/402ea4176fea622ebb2279ada1f94232705de400/vertical-pod-autoscaler/pkg/recommender/logic/recommender.go#L31-L36>90% for target</a>.</p><h3 id=the-weight-of-a-sample>The weight of a sample</h3><p>In a bare histogram, the weight of a bucket is it&rsquo;s ocurrence.
For more sophisticated use-cases, the weight assigned can be tweaked to introduce a &ldquo;bias&rdquo;.
The recommender algorithm introduces the following biases:</p><ol><li>Exponential Decay multiplier</li><li>Load-Adjusted CPU usage multiplier</li></ol><h4 id=exponential-decay-multiplier>Exponential Decay Multiplier</h4><p>The Google Autopilot paper mentions the following rationale behind introducing an exponential decay multiplier:</p><blockquote><p>We want the limits to increase swiftly in response to rising
usage, but reduce slowly after the load decreases to avoid a
too-rapid response to temporary downward workload fluc-
tuations.</p></blockquote><p>To &ldquo;bias&rdquo; the weight of the histogram&rsquo;s samples towards more recent samples, the weight of a sample is multiplied by an <a href=https://en.wikipedia.org/wiki/Exponential_decay>exponentially decaying</a> multiplier:</p>$$2^{\Large {\frac{t - t_0}{\lambda}}}$$<p>where \(t - t_0\) is the relative age of given sample with respect to a reference timestamp,
and \(\lambda\) is the <a href=https://en.wikipedia.org/wiki/Half-life>half-life</a>(24 hours by default).</p><iframe frameborder=0 scrolling=no style=width:100%;height:310px allow=clipboard-write src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fkubernetes%2Fautoscaler%2Fblob%2F402ea4176fea622ebb2279ada1f94232705de400%2Fvertical-pod-autoscaler%2Fpkg%2Frecommender%2Futil%2Fdecaying_histogram.go%23L108-L118&style=atom-one-dark-reasonable&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe><h4 id=load-adjusted-cpu-usage-multiplier>Load-adjusted CPU usage multiplier</h4><p>The reference paper(define properly) mentions the following rationale for using load-adjusted weights for CPU:</p><blockquote><p>In many cases, we want to ensure that a given
percentile of the offered load can be served when the limit
is set to accommodate the offered load, rather than simply
a count of times that instantaneous observed load can be
handled â€“ i.e, we want to weight the calculation by the load,
not the sample count.</p></blockquote><p>A <a href=https://github.com/kubernetes/autoscaler/blob/402ea4176fea622ebb2279ada1f94232705de400/vertical-pod-autoscaler/pkg/recommender/model/aggregate_container_state.go#L207-L222>comment in code</a> explains it the best:
<iframe frameborder=0 scrolling=no style=width:100%;height:184px allow=clipboard-write src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fkubernetes%2Fautoscaler%2Fblob%2F402ea4176fea622ebb2279ada1f94232705de400%2Fvertical-pod-autoscaler%2Fpkg%2Frecommender%2Fmodel%2Faggregate_container_state.go%23L210-L214&style=atom-one-dark-reasonable&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe>
This figure in the reference paper highlights this point visually:
<img src=./vpa_load_adjusted_cpu_figure.png alt="Load-adjusted CPU usage figure"></p><h3 id=safety-margin>Safety margin</h3><p>This is simply a %age margin that the recommended request is scaled by for safety.</p>$$recommendation = (1 + safetyMargin) \times recommendation$$
<iframe frameborder=0 scrolling=no style=width:100%;height:100px allow=clipboard-write src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fkubernetes%2Fautoscaler%2Fblob%2F402ea4176fea622ebb2279ada1f94232705de400%2Fvertical-pod-autoscaler%2Fpkg%2Frecommender%2Flogic%2Festimator.go%23L144&style=atom-one-dark-reasonable&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe><p>Default value: <a href=https://github.com/kubernetes/autoscaler/blob/402ea4176fea622ebb2279ada1f94232705de400/vertical-pod-autoscaler/pkg/recommender/logic/recommender.go#L28>15%</a></p><h3 id=confidence-multiplier>Confidence Multiplier</h3><p>As the VPA recommends resources based on historical resource usage,
if the available data is negligible, the recommendation isn&rsquo;t likely to be correct.
Therefore, the confidence multiplier was introduced.
From a <a href=https://github.com/kubernetes/autoscaler/blob/402ea4176fea622ebb2279ada1f94232705de400/vertical-pod-autoscaler/pkg/recommender/logic/recommender.go#L118-L128>comment in code</a>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#75715e>// ...  This means
</span></span></span><span style=display:flex><span><span style=color:#75715e>// that the updater will be less eager to evict pods with short history
</span></span></span><span style=display:flex><span><span style=color:#75715e>// in order to reclaim unused resources.
</span></span></span></code></pre></div><p>The confidence multiplier is a heuristic defined as:</p>$$\Biggl[{1 + {\frac{multiplier}{confidence}}} \Biggr] ^ {exponent}$$<p>\(multiplier\) and \(exponent\) are heuristic values specified statically in code.</p><p>\(confidence\) is a measure of the available historical data. It is defined as:</p>$$confidence = min(lifespan, samplesAmount)$$<p>where \(lifespan = t_n - t_0\) measured in days,
and \(samplesAmount = {\large \frac{numSamples}{60 \times 24} }\) such that it represents the number of days for which samples are available assuming a rate of 1 sample/minute.<br><iframe frameborder=0 scrolling=no style=width:100%;height:331px allow=clipboard-write src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fkubernetes%2Fautoscaler%2Fblob%2F402ea4176fea622ebb2279ada1f94232705de400%2Fvertical-pod-autoscaler%2Fpkg%2Frecommender%2Flogic%2Frecommender.go%23L118-L129&style=atom-one-dark-reasonable&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe></p><h3 id=upper-bound-and-lower-bound>Upper bound and Lower bound</h3><p>The VPA recommender computes three recommendations with different settings - Lower bound, Target, and Upper bound.
The updater component of the VPA uses this update a pod if the resource request is outside the range \((lowerBound, upperBound)\).</p><iframe frameborder=0 scrolling=no style=width:100%;height:163px allow=clipboard-write src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fkubernetes%2Fautoscaler%2Fblob%2F402ea4176fea622ebb2279ada1f94232705de400%2Fvertical-pod-autoscaler%2Fpkg%2Fupdater%2Fpriority%2Fupdate_priority_calculator.go%23L121-L124&style=atom-one-dark-reasonable&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe><h3 id=minimum-resources>Minimum Resources</h3><p>The recommender imposes a minimum value for both resources - CPU and Memory.
<iframe frameborder=0 scrolling=no style=width:100%;height:184px allow=clipboard-write src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fkubernetes%2Fautoscaler%2Fblob%2F402ea4176fea622ebb2279ada1f94232705de400%2Fvertical-pod-autoscaler%2Fpkg%2Frecommender%2Flogic%2Frecommender.go%23L76-L80&style=atom-one-dark-reasonable&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe></p><p>Default value:</p><ol><li>CPU - <a href=https://github.com/kubernetes/autoscaler/blob/402ea4176fea622ebb2279ada1f94232705de400/vertical-pod-autoscaler/pkg/recommender/logic/recommender.go#L29>0.025 vCPU</a></li><li>Memory - <a href=https://github.com/kubernetes/autoscaler/blob/402ea4176fea622ebb2279ada1f94232705de400/vertical-pod-autoscaler/pkg/recommender/logic/recommender.go#L30>250 MB</a></li></ol><h2 id=conclusion>Conclusion</h2><p>To understand the algorithm, I think the final piece of the puzzle is the considerations for choosing a suitable algorithm that the reference paper mentions.
For memory, the risk of using an incorrect recommendation can lead to OOM errors. For critical workloads, the maximum peak memory usage is considered to avoid any disruption due to an OOM. For less critical ones, percentiles from \(P_{98}\) to even lower percentiles can be used.
For CPU, the risk is much less grave as an incorrect recommendation can result in CPU throttling in the worst case. Thus, we can use a percentile(eg: \(P_{95}\)) or even average if the workload is not CPU-bound.</p><h2 id=references>References</h2><ol><li><a href=https://github.com/kubernetes/autoscaler/issues/2747#issuecomment-616037197>https://github.com/kubernetes/autoscaler/issues/2747#issuecomment-616037197</a></li><li><a href=https://pdfs.semanticscholar.org/74d8/8b2c2bbba11c42439ca9184c8f90f53c43fe.pdf>https://pdfs.semanticscholar.org/74d8/8b2c2bbba11c42439ca9184c8f90f53c43fe.pdf</a></li><li><a href=https://research.google/pubs/autopilot-workload-autoscaling-at-google-scale/>https://research.google/pubs/autopilot-workload-autoscaling-at-google-scale/</a></li></ol><hr><p>Thanks to Shlok for the initial discussion that sparked my interest in the VPA.<br>Thanks to Ashu for encouraging me to explore the VPA.<br>Thanks to Suraj and Gaurav for reviewing this post.</p></content><p><a href=https://ttanay.github.io/tags/algorithms/>#Algorithms</a>
<a href=https://ttanay.github.io/tags/kubernetes/>#Kubernetes</a>
<a href=https://ttanay.github.io/tags/vertical-pod-autoscaler/>#Vertical-Pod-Autoscaler</a></p></main><footer></footer><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script>MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["\\(","\\)"]]}}</script></body></html>